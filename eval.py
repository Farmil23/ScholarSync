
import os
from dotenv import load_dotenv
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
)
from utils import get_llm
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings

# Load Env
load_dotenv()

# Note: Ragas usually requires OpenAI for evaluation metrics (as a judge).
# If you don't have OpenAI key, some metrics might fail or need custom config.
# We will assume OPENAI_API_KEY is present for the 'Judge' LLM, 
# even if the Application LLM is BytePlus.

def run_evaluation():
    print("üß™ Starting Ragas Evaluation...")
    
    # 1. Prepare Test Data
    # In a real scenario, you'd have a Golden Dataset (Questions + Ground Truths).
    # Here we simulate a few samples generated by our system for demonstration.
    
    questions = [
        "What is the main topic of the document?",
        "How does the system handle memory?",
    ]
    
    generated_answers = [
        "The document discusses educational regulations in Indonesia.", # Dummy answer
        "The system uses ConversationBufferMemory to store chat history.", # Dummy answer
    ]
    
    contexts = [
        ["Undang-Undang Nomor 20 Tahun 2003 tentang Sistem Pendidikan Nasional..."], # Dummy Context
        ["Memory is handled via LangChain's memory modules..."], # Dummy Context
    ]
    
    ground_truths = [
        ["The documents cover Indonesian educational laws and regulations."],
        ["It uses a buffer memory mechanism."]
    ]

    data = {
        "question": questions,
        "answer": generated_answers,
        "contexts": contexts,
        "ground_truth": ground_truths
    }
    
    dataset = Dataset.from_dict(data)

    # 2. Run Evaluation
    # Ragas uses OpenAI by default for metrics.
    # We can pass custom LLM if supported, but typically OpenAI Gpt-4 is best for 'Judge'.
    
    try:
        results = evaluate(
            dataset = dataset,
            metrics=[
                faithfulness,
                answer_relevancy,
                context_precision,
            ],
        )
        
        print("\nüìä Evaluation Results:")
        print(results)
        
        # Save results
        df = results.to_pandas()
        df.to_csv("evaluation_results.csv", index=False)
        print("‚úÖ Results saved to evaluation_results.csv")
        
    except Exception as e:
        print(f"‚ùå Evaluation failed: {e}")
        print("Tip: Ragas requires OPENAI_API_KEY for the judge models.")

if __name__ == "__main__":
    run_evaluation()
